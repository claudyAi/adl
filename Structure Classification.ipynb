{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "DuexN6tyOu9k",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Using cached pandas-2.2.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
      "Collecting matplotlib\n",
      "  Using cached matplotlib-3.8.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\n",
      "Collecting opencv-python\n",
      "  Using cached opencv_python-4.9.0.80-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Collecting openpyxl\n",
      "  Using cached openpyxl-3.1.2-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.4.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /opt/conda/lib/python3.11/site-packages (from pandas) (1.26.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas) (2023.3.post1)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Using cached tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Using cached contourpy-1.2.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Using cached fonttools-4.51.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (159 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Using cached kiwisolver-1.4.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (9.3.0)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Using cached pyparsing-3.1.2-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting et-xmlfile (from openpyxl)\n",
      "  Using cached et_xmlfile-1.1.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Using cached scipy-1.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Using cached joblib-1.4.0-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn)\n",
      "  Using cached threadpoolctl-3.4.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Using cached pandas-2.2.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n",
      "Using cached matplotlib-3.8.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
      "Using cached opencv_python-4.9.0.80-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (62.2 MB)\n",
      "Using cached openpyxl-3.1.2-py2.py3-none-any.whl (249 kB)\n",
      "Using cached scikit_learn-1.4.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
      "Using cached contourpy-1.2.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (306 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Using cached fonttools-4.51.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)\n",
      "Using cached joblib-1.4.0-py3-none-any.whl (301 kB)\n",
      "Using cached kiwisolver-1.4.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n",
      "Using cached pyparsing-3.1.2-py3-none-any.whl (103 kB)\n",
      "Using cached scipy-1.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
      "Using cached threadpoolctl-3.4.0-py3-none-any.whl (17 kB)\n",
      "Using cached tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "Using cached et_xmlfile-1.1.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: tzdata, threadpoolctl, scipy, pyparsing, opencv-python, kiwisolver, joblib, fonttools, et-xmlfile, cycler, contourpy, scikit-learn, pandas, openpyxl, matplotlib\n",
      "Successfully installed contourpy-1.2.1 cycler-0.12.1 et-xmlfile-1.1.0 fonttools-4.51.0 joblib-1.4.0 kiwisolver-1.4.5 matplotlib-3.8.4 opencv-python-4.9.0.80 openpyxl-3.1.2 pandas-2.2.2 pyparsing-3.1.2 scikit-learn-1.4.2 scipy-1.13.0 threadpoolctl-3.4.0 tzdata-2024.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas matplotlib opencv-python openpyxl scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "DuexN6tyOu9k"
   },
   "outputs": [],
   "source": [
    "# import os \n",
    "# os.chdir (\"./Jaik's Model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "DuexN6tyOu9k"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "DuexN6tyOu9k"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torchvision.models import resnet34, ResNet34_Weights\n",
    "from torchvision.ops import box_iou\n",
    "from torchvision.transforms import transforms, v2\n",
    "from torchvision.io import read_image, ImageReadMode\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.cuda as cuda\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "import random\n",
    "import math\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from matplotlib.patches import Rectangle\n",
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Zx2hdcTZfG5j"
   },
   "outputs": [],
   "source": [
    "data_dir=Path(\"Dataset for Fetus Framework/Training/Standard\")\n",
    "labels_file=Path(\"ObjectDetection.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "L4xUfo0VzanD"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "L9FzIYzAQVm4"
   },
   "outputs": [],
   "source": [
    "def read_dataset(data_dir, labels_file):\n",
    "    labels_df = pd.read_excel(labels_file)\n",
    "\n",
    "    #Map the different labels to ints in order to train\n",
    "    label_map = {\n",
    "            'thalami': 0,\n",
    "            'nasal bone': 1,\n",
    "            'palate': 2,\n",
    "            'nasal skin': 3,\n",
    "            'nasal tip': 4,\n",
    "            'midbrain': 5,\n",
    "            'NT': 6,\n",
    "            'IT': 7,\n",
    "            'CM': 8 }\n",
    "\n",
    "    #Loop through the excel sheet to make sure each image exist (not all is used)\n",
    "    mask = labels_df.apply(lambda x: os.path.exists(os.path.join(data_dir, x[\"fname\"])), axis=1)\n",
    "    labels_df = labels_df[mask]\n",
    "\n",
    "    #Create image paths in order to read into model\n",
    "    image_paths = [Path(data_dir) / fname for fname in labels_df[\"fname\"]]\n",
    "\n",
    "    #Add all the classes\n",
    "    labels = [label_map[name] for name in labels_df[\"structure\"]]\n",
    "    og_shapes = []\n",
    "\n",
    "    #Progress bar\n",
    "    for image_path in tqdm(image_paths, desc=\"Loading Images\", total=len(image_paths)):\n",
    "        img = cv2.imread(str(image_path))\n",
    "        height, width, _ = img.shape\n",
    "        og_shapes.append((width, height))\n",
    "\n",
    "    #Read bounding boxes\n",
    "    bboxes = labels_df.iloc[:, 2:6].values.astype(float)\n",
    "\n",
    "    #Format data into dataframe\n",
    "    data = {\n",
    "        'filepath': image_paths,\n",
    "        'width': [shape[0] for shape in og_shapes],\n",
    "        'height': [shape[1] for shape in og_shapes],\n",
    "        'class': labels,\n",
    "        'xmin': bboxes[:, 1],\n",
    "        'ymin': bboxes[:, 0],\n",
    "        'xmax': bboxes[:, 3],\n",
    "        'ymax': bboxes[:, 2]\n",
    "    }\n",
    "\n",
    "    df_data = pd.DataFrame(data)\n",
    "\n",
    "    return df_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HwYm2UWzjWUZ",
    "outputId": "9b03707a-af9d-4bfd-86d3-532f9a432c49"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Images: 100%|██████████| 8809/8809 [01:06<00:00, 131.57it/s]\n"
     ]
    }
   ],
   "source": [
    "df_train = read_dataset(data_dir, labels_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "T_BlTpZCuVqf"
   },
   "outputs": [],
   "source": [
    "def read_image(path):\n",
    "    return cv2.imread(str(path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "tSxm4n1TuXmc"
   },
   "outputs": [],
   "source": [
    "def create_mask(bb, x):\n",
    "    #Creates a mask for the bounding box of same shape as image\n",
    "    rows,cols,*_ = x.shape\n",
    "    Y = np.zeros((rows, cols))\n",
    "    bb = bb.astype(np.int32)\n",
    "    Y[bb[0]:bb[2], bb[1]:bb[3]] = 1.\n",
    "    return Y\n",
    "\n",
    "def mask_to_bb(Y):\n",
    "    #Convert mask Y to a bounding box \n",
    "    cols, rows = np.nonzero(Y)\n",
    "    if len(cols)==0:\n",
    "        return np.zeros(4, dtype=np.float32)\n",
    "    top_row = np.min(rows)\n",
    "    left_col = np.min(cols)\n",
    "    bottom_row = np.max(rows)\n",
    "    right_col = np.max(cols)\n",
    "    return np.array([left_col, top_row, right_col, bottom_row], dtype=np.float32)\n",
    "\n",
    "def create_bb_array(x):\n",
    "    #Generates bounding box array from df_train row\n",
    "    return np.array([x[5],x[4],x[7],x[6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "uYwuZCkQubOi"
   },
   "outputs": [],
   "source": [
    "def resize_image_bb(read_path,write_path,bb,sz):\n",
    "    #Resize an image and its bounding box\n",
    "    #Write image to new path (leave original dataset intact)\n",
    "    im = read_image(read_path)\n",
    "    im_resized = cv2.resize(im, (int(1.49*sz), sz))\n",
    "    Y_resized = cv2.resize(create_mask(bb, im), (int(1.49*sz), sz))\n",
    "    new_path = str(write_path/read_path.parts[-1])\n",
    "    cv2.imwrite(new_path, im_resized)\n",
    "    return new_path, mask_to_bb(Y_resized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "id": "CfP8ZQfluceI",
    "outputId": "f314ead4-f1d3-4cd9-ecf5-b0fecdb11094"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resizing Images: 100%|██████████| 8809/8809 [10:49<00:00, 13.56it/s]\n"
     ]
    }
   ],
   "source": [
    "new_paths = []\n",
    "new_bbs = []\n",
    "train_path_resized = Path('image_resized')\n",
    "\n",
    "#Progress Bar\n",
    "for index, row in tqdm(df_train.iterrows(), desc=\"Resizing Images\",total=df_train.shape[0]):\n",
    "    new_path,new_bb = resize_image_bb(row['filepath'], train_path_resized, create_bb_array(row.values),300)\n",
    "    new_paths.append(new_path)\n",
    "    new_bbs.append(new_bb)\n",
    "\n",
    "#Update image path and boxes\n",
    "df_train['new_path'] = new_paths\n",
    "df_train['new_bb'] = new_bbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "13bc5QSRyCcu"
   },
   "outputs": [],
   "source": [
    "def crop(im, r, c, target_r, target_c):\n",
    "    return im[r:r+target_r, c:c+target_c]\n",
    "\n",
    "# Random crop to the original size\n",
    "def random_crop(x, r_pix=8):\n",
    "    r, c,*_ = x.shape\n",
    "    c_pix = round(r_pix*c/r)\n",
    "    rand_r = random.uniform(0, 1)\n",
    "    rand_c = random.uniform(0, 1)\n",
    "    start_r = np.floor(2*rand_r*r_pix).astype(int)\n",
    "    start_c = np.floor(2*rand_c*c_pix).astype(int)\n",
    "    return crop(x, start_r, start_c, r-2*r_pix, c-2*c_pix)\n",
    "\n",
    "def center_crop(x, r_pix=8):\n",
    "    r, c,*_ = x.shape\n",
    "    c_pix = round(r_pix*c/r)\n",
    "    return crop(x, r_pix, c_pix, r-2*r_pix, c-2*c_pix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "BA0kMwWWyc5D"
   },
   "outputs": [],
   "source": [
    "def rotate_cv(im, deg, y=False, mode=cv2.BORDER_REFLECT, interpolation=cv2.INTER_AREA):\n",
    "    #Rotates an image by deg degrees\"\"\"\n",
    "    r,c,*_ = im.shape\n",
    "    M = cv2.getRotationMatrix2D((c/2,r/2),deg,1)\n",
    "    if y:\n",
    "        return cv2.warpAffine(im, M,(c,r), borderMode=cv2.BORDER_CONSTANT)\n",
    "    return cv2.warpAffine(im,M,(c,r), borderMode=mode, flags=cv2.WARP_FILL_OUTLIERS+interpolation)\n",
    "\n",
    "def random_cropXY(x, Y, r_pix=8):\n",
    "    #Returns a random crop\n",
    "    r, c,*_ = x.shape\n",
    "    c_pix = round(r_pix*c/r)\n",
    "    rand_r = random.uniform(0, 1)\n",
    "    rand_c = random.uniform(0, 1)\n",
    "    start_r = np.floor(2*rand_r*r_pix).astype(int)\n",
    "    start_c = np.floor(2*rand_c*c_pix).astype(int)\n",
    "    xx = crop(x, start_r, start_c, r-2*r_pix, c-2*c_pix)\n",
    "    YY = crop(Y, start_r, start_c, r-2*r_pix, c-2*c_pix)\n",
    "    return xx, YY\n",
    "\n",
    "def transformsXY(path, bb, transforms):\n",
    "    #Read image, convert formate, and create a mask\n",
    "    x = cv2.imread(str(path)).astype(np.float32)\n",
    "    x = cv2.cvtColor(x, cv2.COLOR_BGR2RGB)/255\n",
    "    Y = create_mask(bb, x)\n",
    "\n",
    "    #Apply trasformations such as flips and rotates\n",
    "    if transforms:\n",
    "        rdeg = (np.random.random()-.50)*20\n",
    "        x = rotate_cv(x, rdeg)\n",
    "        Y = rotate_cv(Y, rdeg, y=True)\n",
    "        if np.random.random() > 0.5:\n",
    "            x = np.fliplr(x).copy()\n",
    "            Y = np.fliplr(Y).copy()\n",
    "        x, Y = random_cropXY(x, Y)\n",
    "    else:\n",
    "        x, Y = center_crop(x), center_crop(Y)\n",
    "    return x, mask_to_bb(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "rObZ29PJyfUH"
   },
   "outputs": [],
   "source": [
    "def create_corner_rect(bb, color='red'):\n",
    "    bb = np.array(bb, dtype=np.float32)\n",
    "    return plt.Rectangle((bb[1], bb[0]), bb[3]-bb[1], bb[2]-bb[0], color=color,\n",
    "                         fill=False, lw=3)\n",
    "\n",
    "def show_corner_bb(im, bb):\n",
    "    plt.imshow(im)\n",
    "    plt.gca().add_patch(create_corner_rect(bb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "aYIMc2Ggyh_0"
   },
   "outputs": [],
   "source": [
    "df_train = df_train.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "qa58hfTpymFB"
   },
   "outputs": [],
   "source": [
    "#Create training, validation, and test set\n",
    "X_train, X_val_temp, y_train, y_val_temp = train_test_split(df_train[['new_path', 'new_bb']], df_train['class'], test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_val_temp, y_val_temp, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "shm2Q-fwyrno"
   },
   "outputs": [],
   "source": [
    "def normalize(im):\n",
    "    #Normalise images\n",
    "    imagenet_stats = np.array([[0.485, 0.456, 0.406], [0.229, 0.224, 0.225]])\n",
    "    return (im - imagenet_stats[0])/imagenet_stats[1]\n",
    "\n",
    "def unnormalize(im):\n",
    "    #Unnormalise images for printing\n",
    "    imagenet_stats = np.array([[0.485, 0.456, 0.406], [0.229, 0.224, 0.225]])\n",
    "    return im * imagenet_stats[1, np.newaxis, np.newaxis] + imagenet_stats[0, np.newaxis, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "xcbjgAV_yth7"
   },
   "outputs": [],
   "source": [
    "#Pass image data set to edit images\n",
    "class FetusDataset(Dataset):\n",
    "    def __init__(self, paths, bb, y, transforms=False):\n",
    "        self.transforms = transforms\n",
    "        self.paths = paths.values\n",
    "        self.bb = bb.values\n",
    "        self.y = y.values\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.paths[idx]\n",
    "        y_class = self.y[idx]\n",
    "        x, y_bb = transformsXY(path, self.bb[idx], self.transforms)\n",
    "        x = normalize(x)\n",
    "        x = np.rollaxis(x, 2)\n",
    "        return x, y_class, y_bb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "PNNuEtGVyvRM"
   },
   "outputs": [],
   "source": [
    "train_ds = FetusDataset(X_train['new_path'],X_train['new_bb'] ,y_train, transforms=True)\n",
    "valid_ds = FetusDataset(X_val['new_path'],X_val['new_bb'],y_val)\n",
    "test_ds = FetusDataset(X_test['new_path'],X_test['new_bb'],y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "PNNuEtGVyvRM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "506     7\n",
      "2405    6\n",
      "6899    6\n",
      "5156    8\n",
      "621     6\n",
      "       ..\n",
      "5734    6\n",
      "5191    8\n",
      "5390    2\n",
      "860     2\n",
      "7270    0\n",
      "Name: class, Length: 6166, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "EHc13f-PywrB"
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "#Create dataloaders for training set\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=batch_size)\n",
    "test_dl = DataLoader(valid_ds, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "j82334Fpy3iC"
   },
   "outputs": [],
   "source": [
    "#Model to train bounding box\n",
    "class BB_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BB_model, self).__init__()\n",
    "        resnet = models.resnet34(pretrained=True)\n",
    "        \n",
    "        #Retrain last 8 layers\n",
    "        layers = list(resnet.children())[:8]\n",
    "        self.features1 = nn.Sequential(*layers[:6])\n",
    "        self.features2 = nn.Sequential(*layers[6:])\n",
    "        self.classifier = nn.Sequential(nn.BatchNorm1d(512), nn.Linear(512, 4))\n",
    "        \n",
    "        #Train\n",
    "        self.bb = nn.ModuleList([nn.Sequential(nn.BatchNorm1d(512), nn.Linear(512, 4)) for i in range(0,9)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features1(x)\n",
    "        x = self.features2(x)\n",
    "        x = F.leaky_relu(x)\n",
    "        x = nn.AdaptiveAvgPool2d((1,1))(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        bboxes = [self.bb[i](x) for i in range(9)]\n",
    "        return self.classifier(x),bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "uH2a4x15y6Kk"
   },
   "outputs": [],
   "source": [
    "def update_optimizer(optimizer, lr):\n",
    "    for i, param_group in enumerate(optimizer.param_groups):\n",
    "        param_group[\"lr\"] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "VEJzFOM_y7gN"
   },
   "outputs": [],
   "source": [
    "def train_epocs(model, optimizer, train_dl, val_dl, epochs=10,C=1000):\n",
    "    for i in range(epochs):\n",
    "        model.train()\n",
    "        total = 0\n",
    "        sum_loss = 0\n",
    "        for x, y_class, y_bb in tqdm(train_dl, desc=f\"Epoch {i+1}/{epochs}\"):\n",
    "            batch = y_class.shape[0]\n",
    "            x = x.to(device).float()\n",
    "            y_class = y_class.to(device)\n",
    "            y_bb = y_bb.to(device).float()\n",
    "            out_class, boxes = model(x)\n",
    "            for out_bb in boxes:\n",
    "                print(\"I am class\", out_class)\n",
    "                print(\"I am bboxes\", out_bb)\n",
    "                loss_class = F.cross_entropy(out_class, y_class, reduction=\"sum\")\n",
    "                loss_bb = F.l1_loss(out_bb, y_bb, reduction=\"none\").sum(1)\n",
    "                loss_bb = loss_bb.sum()\n",
    "                loss = loss_class + loss_bb/C\n",
    "                sum_loss += loss_class + loss_bb/C\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total += batch\n",
    "            idx += 1\n",
    "        train_loss = sum_loss/total\n",
    "        with torch.no_grad():\n",
    "            val_loss, val_oui, val_dist = val_metrics(model, valid_dl, C)\n",
    "        torch.save(model.state_dict(), 'model_v5.pth')\n",
    "        print(\"train_loss %.3f val_loss %.3f val_oui %.3f val_dist %.3f\" % (train_loss, val_loss, val_oui, val_dist))\n",
    "    return sum_loss/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "PBre-4wMy84U"
   },
   "outputs": [],
   "source": [
    "# def val_metrics(model, valid_dl, C=1000):\n",
    "#     model.eval()\n",
    "#     total = 0\n",
    "#     sum_loss = 0\n",
    "#     correct = 0\n",
    "#     for x, y_class, y_bb in valid_dl:\n",
    "#         batch = y_class.shape[0]\n",
    "#         x = x.to(device).float()\n",
    "#         y_class = y_class.to(device)\n",
    "#         y_bb = y_bb.to(device).float()\n",
    "#         out_class, out_bb = model(x)\n",
    "#         loss_class = F.cross_entropy(out_class, y_class, reduction=\"sum\")\n",
    "#         loss_bb = F.l1_loss(out_bb, y_bb, reduction=\"none\").sum(1)\n",
    "#         loss_bb = loss_bb.sum()\n",
    "#         loss = loss_class + loss_bb/C\n",
    "#         _, pred = torch.max(out_class, 1)\n",
    "#         correct += pred.eq(y_class).sum().item()\n",
    "#         sum_loss += loss.item()\n",
    "#         total += batch\n",
    "#     return sum_loss/total, correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "1MRcmDt25_Oh"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torchvision.ops import box_iou, distance_box_iou\n",
    "\n",
    "def center_xy(bboxes):\n",
    "    #Compute the center (x, y) coordinates of bounding boxes\n",
    "    return (bboxes[:, :2] + bboxes[:, 2:]) / 2\n",
    "\n",
    "def bbox_center_distance(bbox1, bbox2):\n",
    "    #Calculate the distance between the centers of two bounding boxes\n",
    "    center1 = center_xy(bbox1)\n",
    "    center2 = center_xy(bbox2)\n",
    "    return torch.norm(center1 - center2, dim=1)\n",
    "\n",
    "def val_metrics(model, valid_dl, C=1000):\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    sum_loss = 0\n",
    "    iou_total = 0\n",
    "    dist_total = 0\n",
    "    for x, y_class, y_bb in valid_dl:\n",
    "        batch = y_class.shape[0]\n",
    "        x = x.to(device).float()\n",
    "        y_class = y_class.to(device)\n",
    "        y_bb = y_bb.to(device).float()\n",
    "        out_class, bboxes = model(x)\n",
    "        for out_box in bboxes:\n",
    "            loss_class = F.cross_entropy(out_class, y_class, reduction=\"sum\")\n",
    "            loss_bb = F.l1_loss(out_bb, y_bb, reduction=\"sum\")\n",
    "            loss = loss_class + loss_bb/C\n",
    "    \n",
    "            iou = (box_iou(out_bb, y_bb) * torch.eye(batch, device=out_bb.device)).sum()\n",
    "            iou_total += iou\n",
    "    \n",
    "            dist = bbox_center_distance(out_bb, y_bb).sum()\n",
    "            dist_total += dist\n",
    "    \n",
    "            sum_loss += loss.item()\n",
    "            total += batch\n",
    "    return sum_loss/total, iou_total/total, dist/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "SHC6vWryy-aq"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = BB_model().to(device)\n",
    "parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "optimizer = torch.optim.Adam(parameters, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "hjC4pLqqy_dV"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/200:   0%|          | 0/193 [00:00<?, ?it/s]../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [2,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [4,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [6,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [7,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [9,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [13,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [14,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [15,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [18,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [20,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [21,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [22,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [23,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [25,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [26,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [29,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "Epoch 1/200:   0%|          | 0/193 [00:03<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am class tensor([[-0.4151,  1.1974,  0.4022, -1.3287],\n",
      "        [-0.4471, -0.8239, -0.4573,  0.2811],\n",
      "        [-0.4178,  0.3267,  1.2709, -0.4679],\n",
      "        [ 0.2543, -0.1254,  0.2844, -0.2767],\n",
      "        [ 0.6053,  0.1924,  0.6676,  0.3784],\n",
      "        [ 0.2169,  0.0059,  0.1411,  0.4854],\n",
      "        [-0.0925, -0.4387, -1.0907, -0.4802],\n",
      "        [-0.2981, -0.0357,  0.0202,  0.4103],\n",
      "        [-0.3964,  0.5204, -0.0990, -1.7879],\n",
      "        [-0.5021,  0.4064, -0.9764,  1.7072],\n",
      "        [ 0.4019,  0.6651, -0.2396,  0.1875],\n",
      "        [ 0.0721,  0.2811, -0.6778,  0.3941],\n",
      "        [-0.7022, -0.0746,  0.1586, -1.3024],\n",
      "        [ 1.4958, -0.6181,  1.0478, -0.4987],\n",
      "        [ 0.1206, -0.2066, -0.2484,  0.8790],\n",
      "        [ 0.3904, -0.7197,  0.6158,  1.8780],\n",
      "        [-0.1184,  0.9970, -0.2011,  0.0958],\n",
      "        [ 0.3748, -0.4669,  0.4797,  0.6806],\n",
      "        [-0.5895,  0.7461, -0.4311, -0.6002],\n",
      "        [-0.3081,  0.8181, -0.3542, -0.0710],\n",
      "        [-0.3449, -0.3232,  0.1733,  0.2300],\n",
      "        [ 0.9242, -1.2393, -0.3579, -0.4467],\n",
      "        [-0.5761,  0.2136,  0.3275, -0.3780],\n",
      "        [ 0.9816,  0.7592,  0.2657,  0.1407],\n",
      "        [ 0.4297, -0.8241,  0.2971,  0.8961],\n",
      "        [-0.4885, -1.0879, -0.1722, -0.7401],\n",
      "        [ 0.0815,  0.3824,  1.0356, -0.0319],\n",
      "        [ 0.1006,  1.0551, -0.7681,  0.1916],\n",
      "        [-0.3288, -1.1357,  0.7106,  0.7838],\n",
      "        [ 0.3997, -0.4469,  0.6087,  0.7020],\n",
      "        [-0.7439,  0.1982,  0.1410,  0.5844],\n",
      "        [ 0.9763,  0.6166, -1.7162, -1.5314]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "I am bboxes tensor([[-0.5261, -0.2840,  0.5821, -0.9668],\n",
      "        [-0.4752,  1.0589,  0.3459, -0.0054],\n",
      "        [-0.6675,  0.7357, -0.2179, -0.4727],\n",
      "        [-0.7293,  0.5390, -0.1687, -0.5112],\n",
      "        [ 0.6293, -0.7889, -0.1619,  0.1847],\n",
      "        [ 0.1123, -0.0679, -0.3063, -0.4317],\n",
      "        [ 0.2812, -0.3205,  0.4469, -0.4307],\n",
      "        [ 0.6845, -0.3817, -0.4490,  0.5599],\n",
      "        [ 0.0167,  0.6280,  0.3193,  0.2259],\n",
      "        [ 1.2778, -0.5775,  0.5173, -1.4115],\n",
      "        [-0.5602,  0.1388, -0.1491,  0.1377],\n",
      "        [-0.5035,  0.3341, -0.6306,  0.0847],\n",
      "        [-0.0365,  0.6062, -0.9184,  0.0134],\n",
      "        [ 0.1947,  0.0556,  0.8425,  0.3762],\n",
      "        [ 0.5545, -0.0839, -0.0113, -0.0541],\n",
      "        [-0.1572, -0.7352,  0.3168,  0.4291],\n",
      "        [-0.6229, -0.1662, -0.2139,  0.8824],\n",
      "        [-0.2339,  0.2193, -0.6051,  1.2341],\n",
      "        [ 0.3021,  0.6198,  0.5690, -0.1357],\n",
      "        [-0.2955,  0.3148,  0.6164,  0.5547],\n",
      "        [ 0.6355, -0.2719,  0.3775, -0.7775],\n",
      "        [ 0.5691,  0.3029, -0.3741, -0.2964],\n",
      "        [-0.7027,  0.1952, -0.2430, -1.0097],\n",
      "        [ 0.7023, -0.8843, -0.4129,  1.0573],\n",
      "        [-0.2328,  0.6702,  0.3475, -0.5011],\n",
      "        [ 0.6668, -0.6955, -0.5013,  0.8570],\n",
      "        [-0.3560,  0.2557,  0.7224,  0.4749],\n",
      "        [-0.7690, -0.8425, -1.2637, -0.1232],\n",
      "        [ 0.1376, -1.1132, -0.1280, -0.1742],\n",
      "        [ 0.4553,  0.7443,  0.2913,  0.1793],\n",
      "        [ 0.3544, -0.3065,  0.3166,  0.1956],\n",
      "        [-0.6385, -1.1710, -0.5617,  0.1833]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "I am class "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_epocs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[25], line 13\u001b[0m, in \u001b[0;36mtrain_epocs\u001b[0;34m(model, optimizer, train_dl, val_dl, epochs, C)\u001b[0m\n\u001b[1;32m     11\u001b[0m out_class, boxes \u001b[38;5;241m=\u001b[39m model(x)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m out_bb \u001b[38;5;129;01min\u001b[39;00m boxes:\n\u001b[0;32m---> 13\u001b[0m     \u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mI am class\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_class\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI am bboxes\u001b[39m\u001b[38;5;124m\"\u001b[39m, out_bb)\n\u001b[1;32m     15\u001b[0m     loss_class \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(out_class, y_class, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/_tensor.py:431\u001b[0m, in \u001b[0;36mTensor.__repr__\u001b[0;34m(self, tensor_contents)\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    428\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__repr__\u001b[39m, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, tensor_contents\u001b[38;5;241m=\u001b[39mtensor_contents\n\u001b[1;32m    429\u001b[0m     )\n\u001b[1;32m    430\u001b[0m \u001b[38;5;66;03m# All strings are unicode in Python 3.\u001b[39;00m\n\u001b[0;32m--> 431\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tensor_str\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_str\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_contents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_contents\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/_tensor_str.py:664\u001b[0m, in \u001b[0;36m_str\u001b[0;34m(self, tensor_contents)\u001b[0m\n\u001b[1;32m    662\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad(), torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39m_python_dispatch\u001b[38;5;241m.\u001b[39m_disable_current_modes():\n\u001b[1;32m    663\u001b[0m     guard \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_DisableFuncTorch()\n\u001b[0;32m--> 664\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_str_intern\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_contents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_contents\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/_tensor_str.py:595\u001b[0m, in \u001b[0;36m_str_intern\u001b[0;34m(inp, tensor_contents)\u001b[0m\n\u001b[1;32m    593\u001b[0m                     tensor_str \u001b[38;5;241m=\u001b[39m _tensor_str(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_dense(), indent)\n\u001b[1;32m    594\u001b[0m                 \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 595\u001b[0m                     tensor_str \u001b[38;5;241m=\u001b[39m \u001b[43m_tensor_str\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayout \u001b[38;5;241m!=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstrided:\n\u001b[1;32m    598\u001b[0m     suffixes\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlayout=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayout))\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/_tensor_str.py:347\u001b[0m, in \u001b[0;36m_tensor_str\u001b[0;34m(self, indent)\u001b[0m\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _tensor_str_with_formatter(\n\u001b[1;32m    344\u001b[0m         \u001b[38;5;28mself\u001b[39m, indent, summarize, real_formatter, imag_formatter\n\u001b[1;32m    345\u001b[0m     )\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 347\u001b[0m     formatter \u001b[38;5;241m=\u001b[39m \u001b[43m_Formatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_summarized_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msummarize\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _tensor_str_with_formatter(\u001b[38;5;28mself\u001b[39m, indent, summarize, formatter)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/_tensor_str.py:137\u001b[0m, in \u001b[0;36m_Formatter.__init__\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_width \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_width, \u001b[38;5;28mlen\u001b[39m(value_str))\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 137\u001b[0m     nonzero_finite_vals \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmasked_select\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtensor_view\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misfinite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_view\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m&\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtensor_view\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mne\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nonzero_finite_vals\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;66;03m# no valid number, do nothing\u001b[39;00m\n\u001b[1;32m    143\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "train_epocs(model, optimizer, train_dl, valid_dl, epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RA-jTRDF23ZS"
   },
   "outputs": [],
   "source": [
    "def test_model(model, test_dl, device):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, y_class, y_bb in test_dl:\n",
    "            x = x.to(device).float()\n",
    "            y_class = y_class.to(device)\n",
    "            y_bb = y_bb.to(device).float()\n",
    "            out_class, bboxes = model(x)\n",
    "            for out_bb in bboxes:\n",
    "                # Convert output to probabilities and predicted class\n",
    "                probs = F.softmax(out_class, dim=1)\n",
    "                _, preds = torch.max(probs, 1)\n",
    "    \n",
    "                # Get predicted bounding box coordinates\n",
    "                pred_bb = out_bb.cpu().detach().numpy()\n",
    "    \n",
    "                # Plot images with bounding boxes\n",
    "                for i in range(x.shape[0]):\n",
    "                    img = x[i].permute(1, 2, 0).cpu().numpy()  \n",
    "                    img = unnormalize(img)\n",
    "                    true_bbox = y_bb[i].cpu().numpy()\n",
    "    \n",
    "                    # Plot image\n",
    "                    plt.imshow(img)\n",
    "                    ax = plt.gca()\n",
    "    \n",
    "                    # True bounding box\n",
    "                    true_xmin, true_ymin, true_xmax, true_ymax = true_bbox\n",
    "                    true_width = true_xmax - true_xmin\n",
    "                    true_height = true_ymax - true_ymin\n",
    "                    true_rect = plt.Rectangle((true_xmin, true_ymin), true_width, true_height,\n",
    "                                              linewidth=2, edgecolor='g', facecolor='none')\n",
    "                    ax.add_patch(true_rect)\n",
    "    \n",
    "                    # Predicted bounding box\n",
    "                    pred_xmin, pred_ymin, pred_xmax, pred_ymax = pred_bb[i]\n",
    "                    pred_width = pred_xmax - pred_xmin\n",
    "                    pred_height = pred_ymax - pred_ymin\n",
    "                    pred_rect = plt.Rectangle((pred_xmin, pred_ymin), pred_width, pred_height,\n",
    "                                              linewidth=2, edgecolor='r', facecolor='none')\n",
    "                    ax.add_patch(pred_rect)\n",
    "    \n",
    "                    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9B7LaKXI4lvH"
   },
   "outputs": [],
   "source": [
    "test_model(model, test_dl, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
