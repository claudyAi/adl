{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q-3QXJb3DNN7"
   },
   "source": [
    "# SNSClassification\n",
    "Standard and non-standard sagittal view classification of a fetus for Down Syndrome detection.\n",
    "- Output standard label for a high probability of a baby to have downsyndrome. \n",
    "- Output non-standard label for a low probability of a baby to have downsyndrome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ptd2erIBGetn"
   },
   "source": [
    "## Imports\n",
    "Please uncomment the cell below and ensure the necessary packages are downloaded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install openpyxl pycocotools faster-coco-eval torchmetrics[detection] pandas matplotlib tqdm opencv-python cuda-python torchvision \n",
    "# !pip install scikit-learn\n",
    "# !pip install openpyxl\n",
    "# !pip install pandas\n",
    "# !pip install tf_explain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda install pytorch torchvision cudatoolkit=10.1 -c pytorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JzUjM6r1GEYh"
   },
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from IPython.display import clear_output\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "from PIL import Image\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KY8lTs9KGgw3"
   },
   "source": [
    "## Preparing the Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uIPiEpGZ8b7_"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Function to load images and their labels\n",
    "def load_images_and_labels(standard_path, nonstandard_path):\n",
    "    image_paths = []\n",
    "    labels = []  # 1 for Standard, 0 for Non-standard\n",
    "\n",
    "    # Load Non-standard images\n",
    "    for img_name in os.listdir(nonstandard_path):\n",
    "        img_path = os.path.join(nonstandard_path, img_name)\n",
    "        if img_path.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "            image_paths.append(img_path)\n",
    "            labels.append(0)  # Non-standard label\n",
    "\n",
    "    # Load Standard images\n",
    "    for img_name in os.listdir(standard_path):\n",
    "        img_path = os.path.join(standard_path, img_name)\n",
    "        if img_path.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "            image_paths.append(img_path)\n",
    "            labels.append(1)  # Standard label\n",
    "\n",
    "    return image_paths, labels\n",
    "\n",
    "# Define the Dataset class with conditional transformations\n",
    "class FetalUltrasoundDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transform=None, transform_nonstandard=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.transform_nonstandard = transform_nonstandard\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if label == 0 and self.transform_nonstandard:\n",
    "            image = self.transform_nonstandard(image)\n",
    "        elif self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load image paths and labels\n",
    "Please ensure that you download the zip file folders from the [google drive](https://drive.google.com/file/d/1-ppPA9UHw9ZTBxyGmbWEyCgRNKTECC_6/view?usp=drive_link) and add it to the root folder of this directory before running the below code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "standard_path = \"./SNSClassification_Dataset/allStandard\"\n",
    "nonstandard_path = \"./SNSClassification/allNonStandard\"\n",
    "image_paths, labels = load_images_and_labels(standard_path, nonstandard_path)\n",
    "print(f\"Total images loaded: {len(image_paths)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seperate images (perform data augmentations for train + validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_images(image_paths, labels):\n",
    "    standard_images = [img for img, label in zip(image_paths, labels) if label == 1]\n",
    "    nonstandard_images = [img for img, label in zip(image_paths, labels) if label == 0]\n",
    "    return standard_images, nonstandard_images\n",
    "\n",
    "standard_images, nonstandard_images = separate_images(image_paths, labels)\n",
    "print(f\"Number of standard images: {len(standard_images)}\")\n",
    "print(f\"Number of non-standard images: {len(nonstandard_images)}\")\n",
    "\n",
    "# Split for test set (15%)\n",
    "standard_train, standard_test = train_test_split(standard_images, test_size=0.15, random_state=42)\n",
    "nonstandard_train, nonstandard_test = train_test_split(nonstandard_images, test_size=0.15, random_state=42)\n",
    "\n",
    "# Combine for full test set\n",
    "test_images = standard_test + nonstandard_test   #this should have no augmentations done to it \n",
    "test_labels = [1] * len(standard_test) + [0] * len(nonstandard_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## AUGMENTATIONS on NON STANDARD images only to make sure it is balanced with standard images\n",
    "\n",
    "# Augment non-standard images to balance sizes\n",
    "augmented_nonstandard = []\n",
    "augment_transforms = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(degrees=25),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "for path in nonstandard_train:\n",
    "    image = Image.open(path).convert('RGB')\n",
    "    for _ in range((len(standard_train) // len(nonstandard_train)) + 1):  # Augment to balance\n",
    "        augmented_image = augment_transforms(image)\n",
    "        augmented_nonstandard.append((augmented_image, 0))  # Store as a tuple of (image, label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "# Combine and split remaining train and validation sets \n",
    "all_train_images = standard_train + [x[0] for x in augmented_nonstandard]\n",
    "all_train_labels = [1] * len(standard_train) + [0] * len(augmented_nonstandard)\n",
    "\n",
    "print(len(all_train_images), len(all_train_labels))\n",
    "\n",
    "\n",
    "train_images, val_images, train_labels, val_labels = train_test_split(\n",
    "    all_train_images, all_train_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "print(len(train_images), len(val_images))\n",
    "# # Add other necessary imports and class definitions (like FetalUltrasoundDataset)\n",
    "\n",
    "# # Split data\n",
    "image_paths_train, image_paths_temp, labels_train, labels_temp = train_test_split(\n",
    "     image_paths, labels, test_size=0.3, random_state=42)\n",
    "\n",
    "image_paths_val, image_paths_test, labels_val, labels_test = train_test_split(\n",
    "     image_paths_temp, labels_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "\n",
    "# Transformations for the training data with augmentation\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Transformations for the validation and test data without augmentation\n",
    "eval_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = FetalUltrasoundDataset(image_paths_train, labels_train, transform=train_transforms)\n",
    "val_dataset = FetalUltrasoundDataset(image_paths_val, labels_val, transform=eval_transforms)\n",
    "test_dataset = FetalUltrasoundDataset(image_paths_test, labels_test, transform=eval_transforms)\n",
    "\n",
    "\n",
    "# Creating data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Now you can use these loaders in your model training and evaluation routines.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"CUDA is available. Using GPU.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA not available. Using CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D6_smsNhGteL"
   },
   "source": [
    "## Train Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EKiSNsSXOvxK",
    "outputId": "94334227-8c4f-4118-ee1b-51dda0eba38f"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    accuracies = []  # Store accuracies for each epoch\n",
    "    loss_a = []  # Store accuracies for each epoch\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        total = correct = 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_accuracy = 100 * correct / total\n",
    "        accuracies.append(epoch_accuracy)\n",
    "        loss_a.append(epoch_loss)\n",
    "\n",
    "    return accuracies, loss_a\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D6_smsNhGteL"
   },
   "source": [
    "## Evaluate Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UZgXHCP9O8ea"
   },
   "source": [
    "loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install seaborn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def evaluate_model(model, data_loader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total = correct = 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    accuracies = []  # Store accuracies for each epoch\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            # Collect all labels and predictions for confusion matrix\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "            accuracy = 100 * correct / total\n",
    "            print(f'Accuracy: {accuracy:.2f}%')\n",
    "            accuracies.append(accuracy)\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_predictions)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "\n",
    "    # Optionally, plot the confusion matrix using seaborn\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Non-Standard', 'Standard'], yticklabels=['Non-Standard', 'Standard'])\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "    return accuracies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Iteration (ie. Iteration 12) \n",
    "For further testing beyond this iteration, please run Iterations 1 to 11 below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iteration 12 (Resnet 34, Batchsize 16, LR 0.001, epochs 7) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Creating data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load a pretrained ResNet model and modify it for binary classification\n",
    "model = models.resnet34(pretrained=True)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = torch.nn.Linear(num_ftrs, 2)\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "loss, accuracy = train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loss, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For test \n",
    "accur = evaluate_model(model, val_loader)\n",
    "accur_v2 = evaluate_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, 7 + 1), accur, label='Acc Val')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss vs. Epoch')\n",
    "plt.legend()\n",
    "\n",
    "# Plotting the accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, 7 + 1), accur_v2, label='Acc test', color='orange')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Training Accuracy vs. Epoch')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accuracies(accuracies):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.scatter(range(1, len(accuracies) + 1), accuracies)\n",
    "    plt.title('Model Accuracy over Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xticks(range(1, len(accuracies) + 1))\n",
    "    plt.show()\n",
    "\n",
    "# Assume model, train_loader, val_loader, criterion, optimizer are all set up\n",
    "\n",
    "# Train the model and collect accuracies\n",
    "accuracies = train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=50)\n",
    "\n",
    "# Plot the accuracies\n",
    "plot_accuracies(accuracies)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving your model\n",
    "Duplicate and run the below cell whenever you want to use the model. \n",
    "- change params according to iteration you like and rename model accordingly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet34(pretrained=True)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = torch.nn.Linear(num_ftrs, 2)  # Adjust according to your classes\n",
    "torch.save(model.state_dict(), 'BEST.pth')\n",
    "# Load model weights (assuming you've saved your trained model)\n",
    "model.load_state_dict(torch.load('BEST.pth'))\n",
    "model.eval()  # Set the model to evaluation mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other iterations below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iteration 1: Resnet18 + Batchsize 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = models.resnet18(pretrained=True)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = torch.nn.Linear(num_ftrs, 2)\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(model, train_loader, criterion, optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For test \n",
    "evaluate_model(model, val_loader)\n",
    "evaluate_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iteration 2: Resnet 34 + Batchsize 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load a pretrained ResNet model and modify it for binary classification\n",
    "model = models.resnet34(pretrained=True)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = torch.nn.Linear(num_ftrs, 2)\n",
    "model = model.to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "train_model(model, train_loader, criterion, optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For test \n",
    "evaluate_model(model, val_loader)\n",
    "evaluate_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iteration 3: RESNET 34 + Batchsize 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Creating data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load a pretrained ResNet model and modify it for binary classification\n",
    "model = models.resnet34(pretrained=True)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = torch.nn.Linear(num_ftrs, 2)\n",
    "model = model.to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "train_model(model, train_loader, criterion, optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For test \n",
    "evaluate_model(model, val_loader)\n",
    "evaluate_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iteration 4: RESNET 18 + Batchsize 64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create datasets\n",
    "train_dataset = FetalUltrasoundDataset(train_images, train_labels, transform=train_transforms)\n",
    "val_dataset = FetalUltrasoundDataset( val_images, val_labels, transform=train_transforms)\n",
    "test_dataset = FetalUltrasoundDataset(test_images, test_labels, transform=eval_transforms)\n",
    "\n",
    "# Load a pretrained ResNet model and modify it for binary classification\n",
    "model = models.resnet18(pretrained=True)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = torch.nn.Linear(num_ftrs, 2)\n",
    "model = model.to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "train_model(model, train_loader, criterion, optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For test \n",
    "evaluate_model(model, val_loader)\n",
    "evaluate_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iteration 5: RESNET 34 + Batchsize 128\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load a pretrained ResNet model and modify it for binary classification\n",
    "model = models.resnet34(pretrained=True)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = torch.nn.Linear(num_ftrs, 2)\n",
    "model = model.to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "train_model(model, train_loader, criterion, optimizer)\n",
    "\n",
    "\n",
    "# Creating data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For test \n",
    "evaluate_model(model, val_loader)\n",
    "evaluate_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iteration 6: RESNET 34 + Batchsize 64 + Decay 0.01\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load a pretrained ResNet model and modify it for binary classification\n",
    "model = models.resnet34(pretrained=True)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = torch.nn.Linear(num_ftrs, 2)\n",
    "model = model.to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "\n",
    "# Creating data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "train_model(model, train_loader, criterion, optimizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For test \n",
    "evaluate_model(model, val_loader)\n",
    "evaluate_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iteration 7: RESNET 34 + Batchsize 64 + Decay 0.01 + epochs=50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load a pretrained ResNet model and modify it for binary classification\n",
    "model = models.resnet34(pretrained=True)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = torch.nn.Linear(num_ftrs, 2)\n",
    "model = model.to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "\n",
    "# Creating data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "train_model(model, train_loader, criterion, optimizer, num_epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For test \n",
    "evaluate_model(model, val_loader)\n",
    "evaluate_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iteration 8: Resnet 34 + Batchsize64 + Decay 0.001 + epochs=15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pretrained ResNet model and modify it for binary classification\n",
    "model = models.resnet34(pretrained=True)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = torch.nn.Linear(num_ftrs, 2)\n",
    "model = model.to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.001)\n",
    "\n",
    "# Creating data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "train_model(model, train_loader, criterion, optimizer, num_epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For test \n",
    "evaluate_model(model, val_loader)\n",
    "evaluate_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iteration 9: Resnet50 + Batchsize64  + decay 0.01 + epochs=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pretrained ResNet model and modify it for binary classification\n",
    "model = models.resnet50(pretrained=True)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = torch.nn.Linear(num_ftrs, 2)\n",
    "model = model.to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=0.01)\n",
    "\n",
    "# Creating data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "train_model(model, train_loader, criterion, optimizer, num_epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For test \n",
    "evaluate_model(model, val_loader)\n",
    "evaluate_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iteration 10: Resnet50 + Batchsize128  + decay 0.01 + epochs=15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pretrained ResNet model and modify it for binary classification\n",
    "model = models.resnet50(pretrained=True)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = torch.nn.Linear(num_ftrs, 2)\n",
    "model = model.to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=0.01)\n",
    "\n",
    "# Creating data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "train_model(model, train_loader, criterion, optimizer, num_epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For test \n",
    "evaluate_model(model, val_loader)\n",
    "evaluate_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iteration 11: Resnet50 + Batchsize 32 + decay 0.01 + epochs=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pretrained ResNet model and modify it for binary classification\n",
    "model = models.resnet50(pretrained=True)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = torch.nn.Linear(num_ftrs, 2)\n",
    "model = model.to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=0.01)\n",
    "\n",
    "# Creating data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "train_model(model, train_loader, criterion, optimizer, num_epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For test \n",
    "evaluate_model(model, val_loader)\n",
    "evaluate_model(model, test_loader)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
