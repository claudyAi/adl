{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "from torchvision.ops import RoIPool\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from torch.utils.data import random_split\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "# Set PYTORCH_CUDA_ALLOC_CONF environment variable\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FetusDetector(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FetusDetector, self).__init__()\n",
    "        resnet = resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "        layers = list(resnet.children())[:8]\n",
    "        self.features1 = nn.Sequential(*layers[:6])\n",
    "        self.features2 = nn.Sequential(*layers[6:])\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(2084, 81),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.bb = nn.Linear(2084,36)\n",
    "\n",
    "    def forward(self, image, box):\n",
    "        x = self.features1(image)\n",
    "        x = self.features2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        box = box.view(box.size(0), -1)\n",
    "        x = torch.cat((x, box), dim=1)\n",
    "        cls_logits = self.classifier(x)\n",
    "        bbox_logits = self.bb(x)\n",
    "        return cls_logits, bbox_logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FetusDataset(Dataset):\n",
    "    def __init__(self, data_dir, labels_file, transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.labels_df = pd.read_excel(labels_file)\n",
    "        self.transform = transform\n",
    "        self.label_map = {\n",
    "            'thalami': 0,\n",
    "            'nasal bone': 1,\n",
    "            'palate': 2,\n",
    "            'nasal skin': 3,\n",
    "            'nasal tip': 4,\n",
    "            'midbrain': 5,\n",
    "            'NT': 6,\n",
    "            'IT': 7,\n",
    "            'CM': 8\n",
    "        }\n",
    "\n",
    "        self.transform_PIL = transforms.Compose([\n",
    "            transforms.ToPILImage()\n",
    "        ])\n",
    "\n",
    "        self.transform_tensor = transforms.Compose([\n",
    "            transforms.PILToTensor()\n",
    "        ])\n",
    "\n",
    "        # Filter out rows with non-existing image files\n",
    "        self.labels_df = self.labels_df[self.labels_df.apply(lambda x: os.path.exists(os.path.join(self.data_dir, x[\"fname\"])), axis=1)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(os.path.join(self.data_dir, self.labels_df.iloc[idx, 0])).convert('RGB')\n",
    "        image_T = self.transform_tensor(image)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        n_h, n_w = image.shape[1:]\n",
    "\n",
    "        og_h, og_w = image_T.shape[1:]\n",
    "\n",
    "        rows = self.labels_df[self.labels_df.iloc[:, 0] == self.labels_df.iloc[idx, 0]]\n",
    "        bboxes = []\n",
    "        labels = []\n",
    "        for _, row in rows.iterrows():\n",
    "            h_min, w_min, h_max, w_max = row[2:6].values.astype(float)\n",
    "            w_min *= (n_w / og_w)\n",
    "            h_min *= (n_h / og_h)\n",
    "            w_max *= (n_w / og_w)\n",
    "            h_max *= (n_h / og_h)\n",
    "            bboxes.append([w_min, h_min, w_max, h_max])\n",
    "            labels.append(self.label_map.get(row[1]))\n",
    "\n",
    "        bboxes = torch.tensor(bboxes, dtype=torch.float32)\n",
    "        labels = torch.tensor(labels)\n",
    "\n",
    "        # print(bboxes)\n",
    "        # print(labels)\n",
    "\n",
    "        # # Convert image to torch.uint8\n",
    "        # image_s = image.type(torch.uint8)\n",
    "\n",
    "        # # Assuming draw_bounding_boxes is a function provided by torch\n",
    "        # image_box_scaled = draw_bounding_boxes(image_s, boxes=bboxes, labels=[str(x) for x in labels], font=rf\"\\Ariel\\arial.ttf\", font_size=30, width=3, colors=\"red\")\n",
    "\n",
    "        # # Convert the resulting image to PIL\n",
    "        # img = transforms.ToPILImage()(image_box_scaled)\n",
    "\n",
    "        # # Show the image\n",
    "        # plt.imshow(img)\n",
    "        # plt.axis('off')\n",
    "        # plt.show()\n",
    "\n",
    "        return image, bboxes, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths to your data and labels file\n",
    "data_dir = 'Dataset for Fetus Framework\\Training\\Standard'\n",
    "labels_file = 'ObjectDetection.xlsx'\n",
    "\n",
    "# Define transform for data augmentation and normalization\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((470, 650)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load the dataset and split into training and validation sets\n",
    "dataset = FetusDataset(data_dir, labels_file, transform=transform)\n",
    "\n",
    "# Check if any samples are left in the dataset\n",
    "if not dataset:\n",
    "    raise ValueError(\"No valid samples found in the dataset\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate(batch):\n",
    "    images = []\n",
    "    boxes = []\n",
    "    labels = []\n",
    "    max_num_boxes = max(len(data[1]) for data in batch)\n",
    "\n",
    "    for data in batch:\n",
    "        image, bboxes, lbls = data\n",
    "        images.append(image)\n",
    "        num_boxes = len(bboxes)\n",
    "        if num_boxes < max_num_boxes:\n",
    "            pad_boxes = torch.zeros(max_num_boxes - num_boxes, 4)\n",
    "            pad_labels = torch.zeros(max_num_boxes - num_boxes)\n",
    "            bboxes = torch.cat((bboxes, pad_boxes), dim=0)\n",
    "            lbls = torch.cat((lbls, pad_labels), dim=0)\n",
    "        boxes.append(bboxes)\n",
    "        labels.append(lbls)\n",
    "\n",
    "    images = torch.stack(images, dim=0)\n",
    "    boxes = torch.stack(boxes, dim=0)\n",
    "    labels = torch.stack(labels, dim=0)\n",
    "\n",
    "    return images, boxes, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, ground_dataset = random_split(dataset, [0.8, 0.2])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=custom_collate)\n",
    "\n",
    "val_loader = DataLoader(ground_dataset, batch_size=16, shuffle=True, collate_fn=custom_collate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = FetusDetector().to(device)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "scheduler = StepLR(optimizer, step_size=10, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_l1_loss(prediction, target, beta=1.0):\n",
    "\n",
    "    prediction = prediction.view(target.shape)\n",
    "    diff = torch.abs(prediction - target)\n",
    "\n",
    "    smooth_l1_loss = torch.where(diff < beta, 0.5 * diff ** 2 / beta, diff - 0.5 * beta)\n",
    "\n",
    "    loss_per_box = smooth_l1_loss.mean(dim=-1)\n",
    "    loss_per_sample = loss_per_box.mean(dim=-1)\n",
    "\n",
    "    loss = loss_per_sample.mean()\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 40\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_accuracy = 0.0\n",
    "\n",
    "    for batch_idx, (image, boxes, labels) in enumerate(tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}', unit='batch')):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        image, boxes, labels = image.to(device), boxes.to(device), labels.to(device)\n",
    "\n",
    "        # print(boxes.shape)\n",
    "\n",
    "        # # Calculate the number of valid ROIs for each batch\n",
    "        # num_valid_rois = (boxes != 0).sum(dim=1).to(device)\n",
    "\n",
    "        # # Create batch indices\n",
    "        # batch_indices = torch.arange(boxes.size(0)).view(-1, 1, 1).expand(-1, 10, -1).to(device)\n",
    "\n",
    "        # # Mask out the padded regions based on the number of valid ROIs\n",
    "        # padded_mask = (batch_indices < num_valid_rois.unsqueeze(1)).float().to(device)\n",
    "\n",
    "        # # Expand batch indices to match the size of the boxes tensor\n",
    "        # batch_indices_expanded = batch_indices.unsqueeze(-1).expand(-1, -1, -1, 4).to(device)\n",
    "\n",
    "        # # Concatenate batch indices with the boxes tensor and apply the padded mask\n",
    "        # boxes_with_batch_idx = torch.cat((batch_indices_expanded, boxes.unsqueeze(1)), dim=-1) * padded_mask.unsqueeze(-1).to(device)\n",
    "\n",
    "        # # Reshape the tensor to (32 * 10, 5) and remove padding\n",
    "        # boxes_with_batch_idx = boxes_with_batch_idx.view(-1, 5)\n",
    "\n",
    "        # boxes_with_batch_idx = boxes_with_batch_idx.to(device)\n",
    "\n",
    "        # print(boxes_with_batch_idx.shape)\n",
    "\n",
    "        # Forward pass\n",
    "        cls_logits, bbox_logits = model(image, boxes)\n",
    "        # Reshape the output of the classifier to [batch_size * num_boxes, num_classes]\n",
    "        cls_logits_flat = cls_logits.view(-1, 9)\n",
    "\n",
    "        # Reshape the target labels to [batch_size * num_boxes]\n",
    "        # Assuming each bounding box has a single associated class label\n",
    "        labels_flat = labels.view(-1).long()\n",
    "\n",
    "        # Calculate classification loss using CrossEntropyLoss\n",
    "        loss_cls = F.cross_entropy(cls_logits_flat, labels_flat)\n",
    "\n",
    "        # Calculate bounding box regression loss using smooth L1 loss (or any appropriate regression loss)\n",
    "        # Adjust this part based on your specific loss function and requirements\n",
    "        loss_bbox = smooth_l1_loss(bbox_logits, boxes)\n",
    "\n",
    "        # Total loss\n",
    "        loss = loss_cls + loss_bbox\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Compute accuracy\n",
    "        _, predicted = torch.max(cls_logits, 1)\n",
    "        predicted_expanded = predicted.unsqueeze(1).expand_as(labels)\n",
    "        correct = (predicted_expanded == labels).sum().item()\n",
    "        accuracy = correct / (labels.size(0) * labels.size(1))\n",
    "\n",
    "        # Update running loss\n",
    "        running_loss += loss.item()\n",
    "        running_accuracy += accuracy\n",
    "\n",
    "    # Calculate average loss per epoch\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    avg_acc = running_accuracy / len(train_loader)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Avg. Loss: {avg_loss}, Avg. accuracy: {avg_acc}\")\n",
    "\n",
    "    torch.save(model.state_dict(), 'lmao4.pt')\n",
    "\n",
    "    scheduler.step()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ADL_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
