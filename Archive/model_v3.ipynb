{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "#from torchvision.models import resnet50, ResNet50_Weights\n",
    "from torchvision.models import resnet34, ResNet34_Weights\n",
    "from torchvision.ops.box_iou import box_iou\n",
    "from torchvision.transforms import transforms, v2\n",
    "from torchvision.io import read_image, ImageReadMode\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "from torchmetrics.detection import MeanAveragePrecision\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.cuda as cuda\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "# Set PYTORCH_CUDA_ALLOC_CONF environment variable\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FetusDetector(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FetusDetector, self).__init__()\n",
    "        resnet = resnet34(weights=ResNet34_Weights.DEFAULT)\n",
    "        layers = list(resnet.children())[:8]\n",
    "        self.features1 = nn.Sequential(*layers[:6])\n",
    "        self.features2 = nn.Sequential(*layers[6:])\n",
    "        self.bb = nn.Sequential(nn.BatchNorm1d(512), nn.Linear(512, 4))\n",
    "\n",
    "    def forward(self, image, box):\n",
    "        x = self.features1(image)\n",
    "        x = self.features2(x)\n",
    "        x = F.leaky_relu(x)\n",
    "        x = nn.AdaptiveAvgPool2d((1, 1))(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        # x = torch.cat((x, box), dim=1)\n",
    "        bbox_logits = self.bb(x)\n",
    "        return bbox_logits\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import torch\n",
    "# from PIL import Image\n",
    "# from torchvision.io import read_image\n",
    "\n",
    "# data_dir = 'Dataset for Fetus Framework\\Training\\Standard'\n",
    "# labels_file = 'ObjectDetection.xlsx'\n",
    "\n",
    "\n",
    "# # Assuming labels_df contains information about the dataset\n",
    "# # and self.data_dir is the directory where images are stored\n",
    "# labels_df = pd.read_excel(labels_file)\n",
    "\n",
    "# # Filter out rows with non-existing image files\n",
    "# labels_df = labels_df[labels_df.apply(lambda x: os.path.exists(os.path.join(data_dir, x[\"fname\"])), axis=1)]\n",
    "# labels_df = labels_df[labels_df.apply(lambda x: x[\"structure\"] == 'thalami', axis=1)]\n",
    "\n",
    "# # Initialize variables to store minimum height and width\n",
    "# min_height = float('inf')\n",
    "# min_width = float('inf')\n",
    "\n",
    "# # Loop through each image in the dataset\n",
    "# for idx in range(len(labels_df)):\n",
    "#     # Read the image tensor\n",
    "#     image_tensor = read_image(path=os.path.join(data_dir, labels_df.iloc[idx, 0]), mode=ImageReadMode.RGB).float().cuda()\n",
    "\n",
    "#     # Get the height and width of the image tensor\n",
    "#     _, height, width = image_tensor.shape\n",
    "\n",
    "#     # Update minimum height and width if necessary\n",
    "#     min_height = min(min_height, height)\n",
    "#     min_width = min(min_width, width)\n",
    "\n",
    "# # Print the size of the smallest image tensor\n",
    "# print(\"Smallest image tensor size:\", min_height, \"x\", min_width)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FetusDataset(Dataset):\n",
    "    def __init__(self, data_dir, labels_file, transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.labels_df = pd.read_excel(labels_file)\n",
    "        self.transform = transform\n",
    "        self.label_map = {\n",
    "            'thalami': 0,\n",
    "            'nasal bone': 1,\n",
    "            'palate': 2,\n",
    "            'nasal skin': 3,\n",
    "            'nasal tip': 4,\n",
    "            'midbrain': 5,\n",
    "            'NT': 6,\n",
    "            'IT': 7,\n",
    "            'CM': 8\n",
    "        }\n",
    "\n",
    "        self.transform_PIL = transforms.Compose([\n",
    "            transforms.ToPILImage()\n",
    "        ])\n",
    "\n",
    "        self.transform_tensor = transforms.Compose([\n",
    "            transforms.PILToTensor()\n",
    "        ])\n",
    "\n",
    "\n",
    "        # Filter out rows with non-existing image files\n",
    "        self.labels_df = self.labels_df[self.labels_df.apply(lambda x: os.path.exists(os.path.join(self.data_dir, x[\"fname\"])), axis=1)]\n",
    "        self.labels_df = self.labels_df[self.labels_df.apply(lambda x: x[\"structure\"] == 'thalami', axis=1)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = read_image(path=os.path.join(self.data_dir, self.labels_df.iloc[idx, 0]), mode=ImageReadMode.RGB).float().cuda()\n",
    "        # print(image_og)\n",
    "        # print(\"I am imahe tensor in training before transformation\",image)\n",
    "\n",
    "        og_h, og_w = image.shape[1:]\n",
    "        image = self.transform(image)\n",
    "        # print(image)\n",
    "        n_h, n_w = image.shape[1:]\n",
    "        # print(\"I am imahe tensor in training after transformation\",image)\n",
    "\n",
    "        rows = self.labels_df[self.labels_df.iloc[:, 0] == self.labels_df.iloc[idx, 0]]\n",
    "\n",
    "\n",
    "        for _, row in rows.iterrows():\n",
    "            h_min, w_min, h_max, w_max = row[2:6].values.astype(float)\n",
    "            #image_box_unscaled = draw_bounding_boxes(image_og, torch.tensor([[w_min, h_min, w_max, h_max]], dtype=torch.float32), labels=[self.labels_df.iloc[idx, 1]],font=rf\"Ariel\\arial.ttf\",font_size=30 ,width=3, colors=\"red\")\n",
    "            w_min *= (n_w / og_w)\n",
    "            h_min *= (n_h / og_h)\n",
    "            w_max *= (n_w / og_w)\n",
    "            h_max *= (n_h / og_h)\n",
    "            boxes = torch.tensor([[w_min, h_min, w_max, h_max]], dtype=torch.float32)\n",
    "            # label_id = self.label_map.get(row[1])\n",
    "\n",
    "\n",
    "        # image_s = image.type(torch.uint8)\n",
    "        # image_box_scaled = draw_bounding_boxes(image, bboxes, labels=[self.labels_df.iloc[idx, 1]],font=rf\"\\Ariel\\arial.ttf\",font_size=30 ,width=3, colors=\"red\")\n",
    "\n",
    "        # img1 = transforms.ToPILImage()(image_box_unscaled)\n",
    "        # img2 = transforms.ToPILImage()(image_box_scaled)\n",
    "        # img1.show()\n",
    "        # img2.show()\n",
    "\n",
    "\n",
    "        return image, boxes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths to your data and labels file\n",
    "data_dir = 'Dataset for Fetus Framework\\Training\\Standard'\n",
    "labels_file = 'ObjectDetection.xlsx'\n",
    "\n",
    "# Define transform for data augmentation and normalization\n",
    "transform = v2.Compose([\n",
    "    v2.Resize(size=(470, 650))\n",
    "])\n",
    "\n",
    "# Load the dataset and split into training and validation sets\n",
    "dataset = FetusDataset(data_dir, labels_file, transform=transform)\n",
    "\n",
    "# Check if any samples are left in the dataset\n",
    "if not dataset:\n",
    "    raise ValueError(\"No valid samples found in the dataset\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, ground_dataset = random_split(dataset, [0.8, 0.2])\n",
    "\n",
    "train_loader = DataLoader(dataset,batch_size=16, shuffle=True)#, num_workers=12, persistent_workers=True)\n",
    "\n",
    "val_loader = DataLoader(ground_dataset, batch_size=16, shuffle=True)#, num_workers=12, persistent_workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, val_loader, device, map_metric):\n",
    "    model.eval()\n",
    "    map_metric.reset()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for image, boxes in val_loader:\n",
    "            image = image.to(device)\n",
    "            boxes = boxes.squeeze(1).to(device)\n",
    "\n",
    "            bbox_logits = model(image, boxes)\n",
    "            bbox_sizes = (bbox_logits[:, 2] - bbox_logits[:, 0]) * (bbox_logits[:, 3] - bbox_logits[:, 1])\n",
    "            scores = 1 - (bbox_sizes - bbox_sizes.min()) / (bbox_sizes.max() - bbox_sizes.min())\n",
    "            scores_tensor = torch.tensor(scores, dtype=torch.float32, device=device)\n",
    "            scores_tensor = scores_tensor.squeeze()\n",
    "\n",
    "            preds_list = []\n",
    "            target_list = []\n",
    "\n",
    "            label_tensor = torch.tensor([0], device=device)\n",
    "\n",
    "            for pred_box, target_box, score in zip(bbox_logits, boxes, scores_tensor):\n",
    "                pred_dict = {\n",
    "                    \"boxes\": pred_box.unsqueeze(0),\n",
    "                    \"scores\": torch.tensor([score], dtype=torch.float32, device=device),\n",
    "                    \"labels\": label_tensor.repeat(len(pred_box.unsqueeze(0)))\n",
    "                }\n",
    "                preds_list.append(pred_dict)\n",
    "\n",
    "                target_dict = {\n",
    "                    \"boxes\": target_box.unsqueeze(0),\n",
    "                    \"labels\": label_tensor.repeat(len(target_box.unsqueeze(0)))\n",
    "                }\n",
    "                target_list.append(target_dict)\n",
    "\n",
    "            map_metric.update(preds=preds_list, target=target_list)\n",
    "\n",
    "    map_value = map_metric.compute()\n",
    "    map_metric.reset()\n",
    "\n",
    "    return map_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_l1_loss(prediction, target, beta=1.0):\n",
    "    diff = torch.abs(prediction - target)\n",
    "    smooth_l1_loss = torch.where(diff < beta, 0.5 * diff ** 2 / beta, diff - 0.5 * beta)\n",
    "    return smooth_l1_loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_mean(model, train_loader, optimizer, scheduler, device,num_epochs=10):\n",
    "    num_epochs = 10\n",
    "    map_metric = MeanAveragePrecision(box_format='xyxy', iou_type='bbox')\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}', leave=False)\n",
    "\n",
    "        for batch_idx, (image, boxes) in enumerate(pbar):\n",
    "            optimizer.zero_grad()\n",
    "            label_tensor = torch.tensor([0], device='cuda:0')\n",
    "            image = image.cuda().float()\n",
    "            boxes = boxes.squeeze(1).cuda().float()\n",
    "            label_tensor = label_tensor.cuda().float()\n",
    "\n",
    "            bbox_logits = model(image, boxes)\n",
    "            bbox_sizes = (bbox_logits[:, 2] - bbox_logits[:, 0]) * (bbox_logits[:, 3] - bbox_logits[:, 1])\n",
    "            scores = 1 - (bbox_sizes - bbox_sizes.min()) / (bbox_sizes.max() - bbox_sizes.min())\n",
    "            scores_tensor = torch.tensor(scores, dtype=torch.float32, device=device)\n",
    "            scores_tensor = scores_tensor.squeeze()\n",
    "\n",
    "            preds_list = []\n",
    "            target_list = []\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for pred_box, target_box, score in zip(bbox_logits, boxes, scores_tensor):\n",
    "                    pred_dict = {\n",
    "                        \"boxes\": pred_box.unsqueeze(0),\n",
    "                        \"scores\": torch.tensor([score], dtype=torch.float32, device=device),\n",
    "                        \"labels\": label_tensor.repeat(len(pred_box.unsqueeze(0)))\n",
    "                    }\n",
    "                    preds_list.append(pred_dict)\n",
    "\n",
    "                    target_dict = {\n",
    "                        \"boxes\": target_box.unsqueeze(0),\n",
    "                        \"labels\": label_tensor.repeat(len(target_box.unsqueeze(0)))\n",
    "                    }\n",
    "                    target_list.append(target_dict)\n",
    "\n",
    "            map_metric.update(preds=preds_list, target=target_list)\n",
    "\n",
    "            loss_bbox = smooth_l1_loss(bbox_logits, boxes)\n",
    "            loss = loss_bbox\n",
    "\n",
    "            loss.backward()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            loss = loss.detach().cpu().item()\n",
    "            pbar.set_postfix(loss=loss)\n",
    "\n",
    "        # Compute mAP after each epoch\n",
    "        print(map_metric)\n",
    "        map_value = map_metric.compute()\n",
    "        map_metric.reset()\n",
    "\n",
    "        avg_loss = running_loss / len(train_loader.dataset)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}, Train Avg. Loss: {avg_loss}, Mean Average Precision: {map_value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_bbox_metrics(model, val_loader):\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    sum_iou = 0\n",
    "    for image, boxes in val_loader:\n",
    "        image = image.cuda().float()\n",
    "        boxes = boxes.squeeze(1).cuda().float()\n",
    "        pred_boxes = model(image, boxes)\n",
    "        batch = boxes.shape[0]\n",
    "        for pred_box, true_box in zip(pred_boxes, boxes):\n",
    "            iou = box_iou(pred_box.unsqueeze(0), true_box.unsqueeze(0))\n",
    "            sum_iou += iou.item()\n",
    "        total += batch\n",
    "    return sum_iou / total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def val_metrics(model, val_loader, batch_size):\n",
    "#     model.eval()\n",
    "#     total = 0\n",
    "#     correct = 0\n",
    "#     for image, boxes in val_loader:\n",
    "#         image = image.cuda()\n",
    "#         boxes = boxes.squeeze(1).cuda()\n",
    "#         out_bb = model(image, boxes)\n",
    "#         pred_bb = out_bb.round()\n",
    "#         pred_bb_cpu = pred_bb.cpu()\n",
    "#         boxes_cpu = boxes.cpu()\n",
    "#         correct += (pred_bb_cpu == boxes_cpu).all(dim=1).sum().item()\n",
    "#         total += batch_size\n",
    "#     return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def val_metrics(model, val_loader, batch_size):\n",
    "#     model.eval()\n",
    "#     total_correct = 0\n",
    "#     total_samples = 0\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for images, boxes in val_loader:\n",
    "#             images = images.cuda().float()\n",
    "#             boxes = boxes.cuda().float()\n",
    "\n",
    "#             out_bb = model(images, boxes)\n",
    "#             pred_bb = torch.round(out_bb)\n",
    "#             correct = torch.all(pred_bb == boxes, dim=1).sum().item()\n",
    "\n",
    "#             total_correct += correct\n",
    "#             total_samples += images.size(0)\n",
    "\n",
    "#     accuracy = total_correct / total_samples if total_samples > 0 else 0\n",
    "#     return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, optimizer, scheduler, device,num_epochs=10):\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        cuda.empty_cache()\n",
    "        torch.cuda.memory_reserved()\n",
    "\n",
    "        pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}', leave=False)\n",
    "\n",
    "        for batch_idx, (image, boxes) in enumerate(pbar):\n",
    "            optimizer.zero_grad()\n",
    "            label_tensor = torch.tensor([0], device=device)\n",
    "            image = image.cuda().float()\n",
    "            boxes = boxes.squeeze(1).cuda().float()\n",
    "            label_tensor = label_tensor.cuda()\n",
    "\n",
    "            bbox_logits = model(image, boxes)\n",
    "\n",
    "            loss_bbox = smooth_l1_loss(bbox_logits, boxes)\n",
    "            loss = loss_bbox\n",
    "\n",
    "            loss.backward()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            loss = loss.detach().cpu().item()\n",
    "            pbar.set_postfix(loss=loss)\n",
    "\n",
    "        cuda.empty_cache()\n",
    "        torch.cuda.memory_reserved()\n",
    "\n",
    "        accuracy = val_bbox_metrics(model, val_loader)\n",
    "\n",
    "        torch.save(model.state_dict(), 'lmao6.pt')\n",
    "        print(f\"Epoch {epoch+1}, Validation Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\midni\\anaconda3\\envs\\ADL_project\\lib\\site-packages\\torch\\nn\\modules\\module.py:1150: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\c10/cuda/CUDAAllocatorConfig.h:30.)\n",
      "  return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\n",
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Validation Accuracy: 1059.0742268880208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = FetusDetector().to(device)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "scheduler = StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "train_model(model, train_loader, optimizer, scheduler, device, num_epochs=10)\n",
    "#train_model_mean(model, train_loader, optimizer, scheduler, device, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for FetusDetector:\n\tUnexpected key(s) in state_dict: \"features1.4.0.conv3.weight\", \"features1.4.0.bn3.weight\", \"features1.4.0.bn3.bias\", \"features1.4.0.bn3.running_mean\", \"features1.4.0.bn3.running_var\", \"features1.4.0.bn3.num_batches_tracked\", \"features1.4.0.downsample.0.weight\", \"features1.4.0.downsample.1.weight\", \"features1.4.0.downsample.1.bias\", \"features1.4.0.downsample.1.running_mean\", \"features1.4.0.downsample.1.running_var\", \"features1.4.0.downsample.1.num_batches_tracked\", \"features1.4.1.conv3.weight\", \"features1.4.1.bn3.weight\", \"features1.4.1.bn3.bias\", \"features1.4.1.bn3.running_mean\", \"features1.4.1.bn3.running_var\", \"features1.4.1.bn3.num_batches_tracked\", \"features1.4.2.conv3.weight\", \"features1.4.2.bn3.weight\", \"features1.4.2.bn3.bias\", \"features1.4.2.bn3.running_mean\", \"features1.4.2.bn3.running_var\", \"features1.4.2.bn3.num_batches_tracked\", \"features1.5.0.conv3.weight\", \"features1.5.0.bn3.weight\", \"features1.5.0.bn3.bias\", \"features1.5.0.bn3.running_mean\", \"features1.5.0.bn3.running_var\", \"features1.5.0.bn3.num_batches_tracked\", \"features1.5.1.conv3.weight\", \"features1.5.1.bn3.weight\", \"features1.5.1.bn3.bias\", \"features1.5.1.bn3.running_mean\", \"features1.5.1.bn3.running_var\", \"features1.5.1.bn3.num_batches_tracked\", \"features1.5.2.conv3.weight\", \"features1.5.2.bn3.weight\", \"features1.5.2.bn3.bias\", \"features1.5.2.bn3.running_mean\", \"features1.5.2.bn3.running_var\", \"features1.5.2.bn3.num_batches_tracked\", \"features1.5.3.conv3.weight\", \"features1.5.3.bn3.weight\", \"features1.5.3.bn3.bias\", \"features1.5.3.bn3.running_mean\", \"features1.5.3.bn3.running_var\", \"features1.5.3.bn3.num_batches_tracked\", \"features2.0.0.conv3.weight\", \"features2.0.0.bn3.weight\", \"features2.0.0.bn3.bias\", \"features2.0.0.bn3.running_mean\", \"features2.0.0.bn3.running_var\", \"features2.0.0.bn3.num_batches_tracked\", \"features2.0.1.conv3.weight\", \"features2.0.1.bn3.weight\", \"features2.0.1.bn3.bias\", \"features2.0.1.bn3.running_mean\", \"features2.0.1.bn3.running_var\", \"features2.0.1.bn3.num_batches_tracked\", \"features2.0.2.conv3.weight\", \"features2.0.2.bn3.weight\", \"features2.0.2.bn3.bias\", \"features2.0.2.bn3.running_mean\", \"features2.0.2.bn3.running_var\", \"features2.0.2.bn3.num_batches_tracked\", \"features2.0.3.conv3.weight\", \"features2.0.3.bn3.weight\", \"features2.0.3.bn3.bias\", \"features2.0.3.bn3.running_mean\", \"features2.0.3.bn3.running_var\", \"features2.0.3.bn3.num_batches_tracked\", \"features2.0.4.conv3.weight\", \"features2.0.4.bn3.weight\", \"features2.0.4.bn3.bias\", \"features2.0.4.bn3.running_mean\", \"features2.0.4.bn3.running_var\", \"features2.0.4.bn3.num_batches_tracked\", \"features2.0.5.conv3.weight\", \"features2.0.5.bn3.weight\", \"features2.0.5.bn3.bias\", \"features2.0.5.bn3.running_mean\", \"features2.0.5.bn3.running_var\", \"features2.0.5.bn3.num_batches_tracked\", \"features2.1.0.conv3.weight\", \"features2.1.0.bn3.weight\", \"features2.1.0.bn3.bias\", \"features2.1.0.bn3.running_mean\", \"features2.1.0.bn3.running_var\", \"features2.1.0.bn3.num_batches_tracked\", \"features2.1.1.conv3.weight\", \"features2.1.1.bn3.weight\", \"features2.1.1.bn3.bias\", \"features2.1.1.bn3.running_mean\", \"features2.1.1.bn3.running_var\", \"features2.1.1.bn3.num_batches_tracked\", \"features2.1.2.conv3.weight\", \"features2.1.2.bn3.weight\", \"features2.1.2.bn3.bias\", \"features2.1.2.bn3.running_mean\", \"features2.1.2.bn3.running_var\", \"features2.1.2.bn3.num_batches_tracked\". \n\tsize mismatch for features1.4.0.conv1.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 3, 3]).\n\tsize mismatch for features1.4.1.conv1.weight: copying a param with shape torch.Size([64, 256, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 3, 3]).\n\tsize mismatch for features1.4.2.conv1.weight: copying a param with shape torch.Size([64, 256, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 3, 3]).\n\tsize mismatch for features1.5.0.conv1.weight: copying a param with shape torch.Size([128, 256, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 64, 3, 3]).\n\tsize mismatch for features1.5.0.downsample.0.weight: copying a param with shape torch.Size([512, 256, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 64, 1, 1]).\n\tsize mismatch for features1.5.0.downsample.1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for features1.5.0.downsample.1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for features1.5.0.downsample.1.running_mean: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for features1.5.0.downsample.1.running_var: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for features1.5.1.conv1.weight: copying a param with shape torch.Size([128, 512, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 3, 3]).\n\tsize mismatch for features1.5.2.conv1.weight: copying a param with shape torch.Size([128, 512, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 3, 3]).\n\tsize mismatch for features1.5.3.conv1.weight: copying a param with shape torch.Size([128, 512, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 3, 3]).\n\tsize mismatch for features2.0.0.conv1.weight: copying a param with shape torch.Size([256, 512, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 128, 3, 3]).\n\tsize mismatch for features2.0.0.downsample.0.weight: copying a param with shape torch.Size([1024, 512, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 128, 1, 1]).\n\tsize mismatch for features2.0.0.downsample.1.weight: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for features2.0.0.downsample.1.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for features2.0.0.downsample.1.running_mean: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for features2.0.0.downsample.1.running_var: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for features2.0.1.conv1.weight: copying a param with shape torch.Size([256, 1024, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 3, 3]).\n\tsize mismatch for features2.0.2.conv1.weight: copying a param with shape torch.Size([256, 1024, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 3, 3]).\n\tsize mismatch for features2.0.3.conv1.weight: copying a param with shape torch.Size([256, 1024, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 3, 3]).\n\tsize mismatch for features2.0.4.conv1.weight: copying a param with shape torch.Size([256, 1024, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 3, 3]).\n\tsize mismatch for features2.0.5.conv1.weight: copying a param with shape torch.Size([256, 1024, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 3, 3]).\n\tsize mismatch for features2.1.0.conv1.weight: copying a param with shape torch.Size([512, 1024, 1, 1]) from checkpoint, the shape in current model is torch.Size([512, 256, 3, 3]).\n\tsize mismatch for features2.1.0.downsample.0.weight: copying a param with shape torch.Size([2048, 1024, 1, 1]) from checkpoint, the shape in current model is torch.Size([512, 256, 1, 1]).\n\tsize mismatch for features2.1.0.downsample.1.weight: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for features2.1.0.downsample.1.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for features2.1.0.downsample.1.running_mean: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for features2.1.0.downsample.1.running_var: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for features2.1.1.conv1.weight: copying a param with shape torch.Size([512, 2048, 1, 1]) from checkpoint, the shape in current model is torch.Size([512, 512, 3, 3]).\n\tsize mismatch for features2.1.2.conv1.weight: copying a param with shape torch.Size([512, 2048, 1, 1]) from checkpoint, the shape in current model is torch.Size([512, 512, 3, 3]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m state_dict \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlmao6.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m val_metrics(model, val_loader, \u001b[38;5;241m16\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\midni\\anaconda3\\envs\\ADL_project\\lib\\site-packages\\torch\\nn\\modules\\module.py:2153\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[0;32m   2148\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[0;32m   2149\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2150\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[0;32m   2152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2153\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2154\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[0;32m   2155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for FetusDetector:\n\tUnexpected key(s) in state_dict: \"features1.4.0.conv3.weight\", \"features1.4.0.bn3.weight\", \"features1.4.0.bn3.bias\", \"features1.4.0.bn3.running_mean\", \"features1.4.0.bn3.running_var\", \"features1.4.0.bn3.num_batches_tracked\", \"features1.4.0.downsample.0.weight\", \"features1.4.0.downsample.1.weight\", \"features1.4.0.downsample.1.bias\", \"features1.4.0.downsample.1.running_mean\", \"features1.4.0.downsample.1.running_var\", \"features1.4.0.downsample.1.num_batches_tracked\", \"features1.4.1.conv3.weight\", \"features1.4.1.bn3.weight\", \"features1.4.1.bn3.bias\", \"features1.4.1.bn3.running_mean\", \"features1.4.1.bn3.running_var\", \"features1.4.1.bn3.num_batches_tracked\", \"features1.4.2.conv3.weight\", \"features1.4.2.bn3.weight\", \"features1.4.2.bn3.bias\", \"features1.4.2.bn3.running_mean\", \"features1.4.2.bn3.running_var\", \"features1.4.2.bn3.num_batches_tracked\", \"features1.5.0.conv3.weight\", \"features1.5.0.bn3.weight\", \"features1.5.0.bn3.bias\", \"features1.5.0.bn3.running_mean\", \"features1.5.0.bn3.running_var\", \"features1.5.0.bn3.num_batches_tracked\", \"features1.5.1.conv3.weight\", \"features1.5.1.bn3.weight\", \"features1.5.1.bn3.bias\", \"features1.5.1.bn3.running_mean\", \"features1.5.1.bn3.running_var\", \"features1.5.1.bn3.num_batches_tracked\", \"features1.5.2.conv3.weight\", \"features1.5.2.bn3.weight\", \"features1.5.2.bn3.bias\", \"features1.5.2.bn3.running_mean\", \"features1.5.2.bn3.running_var\", \"features1.5.2.bn3.num_batches_tracked\", \"features1.5.3.conv3.weight\", \"features1.5.3.bn3.weight\", \"features1.5.3.bn3.bias\", \"features1.5.3.bn3.running_mean\", \"features1.5.3.bn3.running_var\", \"features1.5.3.bn3.num_batches_tracked\", \"features2.0.0.conv3.weight\", \"features2.0.0.bn3.weight\", \"features2.0.0.bn3.bias\", \"features2.0.0.bn3.running_mean\", \"features2.0.0.bn3.running_var\", \"features2.0.0.bn3.num_batches_tracked\", \"features2.0.1.conv3.weight\", \"features2.0.1.bn3.weight\", \"features2.0.1.bn3.bias\", \"features2.0.1.bn3.running_mean\", \"features2.0.1.bn3.running_var\", \"features2.0.1.bn3.num_batches_tracked\", \"features2.0.2.conv3.weight\", \"features2.0.2.bn3.weight\", \"features2.0.2.bn3.bias\", \"features2.0.2.bn3.running_mean\", \"features2.0.2.bn3.running_var\", \"features2.0.2.bn3.num_batches_tracked\", \"features2.0.3.conv3.weight\", \"features2.0.3.bn3.weight\", \"features2.0.3.bn3.bias\", \"features2.0.3.bn3.running_mean\", \"features2.0.3.bn3.running_var\", \"features2.0.3.bn3.num_batches_tracked\", \"features2.0.4.conv3.weight\", \"features2.0.4.bn3.weight\", \"features2.0.4.bn3.bias\", \"features2.0.4.bn3.running_mean\", \"features2.0.4.bn3.running_var\", \"features2.0.4.bn3.num_batches_tracked\", \"features2.0.5.conv3.weight\", \"features2.0.5.bn3.weight\", \"features2.0.5.bn3.bias\", \"features2.0.5.bn3.running_mean\", \"features2.0.5.bn3.running_var\", \"features2.0.5.bn3.num_batches_tracked\", \"features2.1.0.conv3.weight\", \"features2.1.0.bn3.weight\", \"features2.1.0.bn3.bias\", \"features2.1.0.bn3.running_mean\", \"features2.1.0.bn3.running_var\", \"features2.1.0.bn3.num_batches_tracked\", \"features2.1.1.conv3.weight\", \"features2.1.1.bn3.weight\", \"features2.1.1.bn3.bias\", \"features2.1.1.bn3.running_mean\", \"features2.1.1.bn3.running_var\", \"features2.1.1.bn3.num_batches_tracked\", \"features2.1.2.conv3.weight\", \"features2.1.2.bn3.weight\", \"features2.1.2.bn3.bias\", \"features2.1.2.bn3.running_mean\", \"features2.1.2.bn3.running_var\", \"features2.1.2.bn3.num_batches_tracked\". \n\tsize mismatch for features1.4.0.conv1.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 3, 3]).\n\tsize mismatch for features1.4.1.conv1.weight: copying a param with shape torch.Size([64, 256, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 3, 3]).\n\tsize mismatch for features1.4.2.conv1.weight: copying a param with shape torch.Size([64, 256, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 3, 3]).\n\tsize mismatch for features1.5.0.conv1.weight: copying a param with shape torch.Size([128, 256, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 64, 3, 3]).\n\tsize mismatch for features1.5.0.downsample.0.weight: copying a param with shape torch.Size([512, 256, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 64, 1, 1]).\n\tsize mismatch for features1.5.0.downsample.1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for features1.5.0.downsample.1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for features1.5.0.downsample.1.running_mean: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for features1.5.0.downsample.1.running_var: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for features1.5.1.conv1.weight: copying a param with shape torch.Size([128, 512, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 3, 3]).\n\tsize mismatch for features1.5.2.conv1.weight: copying a param with shape torch.Size([128, 512, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 3, 3]).\n\tsize mismatch for features1.5.3.conv1.weight: copying a param with shape torch.Size([128, 512, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 3, 3]).\n\tsize mismatch for features2.0.0.conv1.weight: copying a param with shape torch.Size([256, 512, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 128, 3, 3]).\n\tsize mismatch for features2.0.0.downsample.0.weight: copying a param with shape torch.Size([1024, 512, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 128, 1, 1]).\n\tsize mismatch for features2.0.0.downsample.1.weight: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for features2.0.0.downsample.1.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for features2.0.0.downsample.1.running_mean: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for features2.0.0.downsample.1.running_var: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for features2.0.1.conv1.weight: copying a param with shape torch.Size([256, 1024, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 3, 3]).\n\tsize mismatch for features2.0.2.conv1.weight: copying a param with shape torch.Size([256, 1024, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 3, 3]).\n\tsize mismatch for features2.0.3.conv1.weight: copying a param with shape torch.Size([256, 1024, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 3, 3]).\n\tsize mismatch for features2.0.4.conv1.weight: copying a param with shape torch.Size([256, 1024, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 3, 3]).\n\tsize mismatch for features2.0.5.conv1.weight: copying a param with shape torch.Size([256, 1024, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 3, 3]).\n\tsize mismatch for features2.1.0.conv1.weight: copying a param with shape torch.Size([512, 1024, 1, 1]) from checkpoint, the shape in current model is torch.Size([512, 256, 3, 3]).\n\tsize mismatch for features2.1.0.downsample.0.weight: copying a param with shape torch.Size([2048, 1024, 1, 1]) from checkpoint, the shape in current model is torch.Size([512, 256, 1, 1]).\n\tsize mismatch for features2.1.0.downsample.1.weight: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for features2.1.0.downsample.1.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for features2.1.0.downsample.1.running_mean: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for features2.1.0.downsample.1.running_var: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for features2.1.1.conv1.weight: copying a param with shape torch.Size([512, 2048, 1, 1]) from checkpoint, the shape in current model is torch.Size([512, 512, 3, 3]).\n\tsize mismatch for features2.1.2.conv1.weight: copying a param with shape torch.Size([512, 2048, 1, 1]) from checkpoint, the shape in current model is torch.Size([512, 512, 3, 3])."
     ]
    }
   ],
   "source": [
    "# state_dict = torch.load('lmao6.pt')\n",
    "# model.load_state_dict(state_dict)\n",
    "# val_metrics(model, val_loader, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|          | 0/66 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\midni\\AppData\\Local\\Temp\\ipykernel_14552\\2836204004.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  scores_tensor = torch.tensor(scores, dtype=torch.float32, device=device)\n",
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Avg. Loss: 16.606184547061012, Mean Average Precision: {'map': tensor(0.), 'map_50': tensor(0.), 'map_75': tensor(0.), 'map_small': tensor(-1.), 'map_medium': tensor(0.), 'map_large': tensor(0.), 'mar_1': tensor(0.), 'mar_10': tensor(0.), 'mar_100': tensor(0.), 'mar_small': tensor(-1.), 'mar_medium': tensor(0.), 'mar_large': tensor(0.), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor(0, dtype=torch.int32)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Train Avg. Loss: 16.60822525751023, Mean Average Precision: {'map': tensor(0.), 'map_50': tensor(0.), 'map_75': tensor(0.), 'map_small': tensor(-1.), 'map_medium': tensor(0.), 'map_large': tensor(0.), 'mar_1': tensor(0.), 'mar_10': tensor(0.), 'mar_100': tensor(0.), 'mar_small': tensor(-1.), 'mar_medium': tensor(0.), 'mar_large': tensor(0.), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor(0, dtype=torch.int32)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Train Avg. Loss: 16.601668584914435, Mean Average Precision: {'map': tensor(0.), 'map_50': tensor(0.), 'map_75': tensor(0.), 'map_small': tensor(-1.), 'map_medium': tensor(0.), 'map_large': tensor(0.), 'mar_1': tensor(0.), 'mar_10': tensor(0.), 'mar_100': tensor(0.), 'mar_small': tensor(-1.), 'mar_medium': tensor(0.), 'mar_large': tensor(0.), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor(0, dtype=torch.int32)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10:  33%|███▎      | 22/66 [06:02<12:30, 17.05s/it, loss=266]"
     ]
    }
   ],
   "source": [
    "# num_epochs = 10\n",
    "# map_metric = MeanAveragePrecision(box_format='xyxy', iou_type='bbox')\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     model.train()\n",
    "#     running_loss = 0.0\n",
    "#     scores_list = []\n",
    "\n",
    "#     pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}', leave=False)\n",
    "\n",
    "#     for batch_idx, (image, boxes) in enumerate(pbar):\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         label_tensor = torch.tensor([0], device='cuda:0')\n",
    "\n",
    "#         image = image.to(device)\n",
    "#         boxes = boxes.squeeze(1).to(device)\n",
    "#         #boxes = boxes.to(device)\n",
    "#         label_tensor = label_tensor.to(device)\n",
    "\n",
    "#         # batch_index = torch.tensor([[batch_idx] * len(boxes)]).to(device)\n",
    "#         # batch_index = batch_index.view(-1, 1)\n",
    "#         # boxes_with_index = torch.cat((batch_index, boxes), dim=1).to(device)\n",
    "\n",
    "#         # print(image)\n",
    "#         # print(boxes_with_index)\n",
    "\n",
    "#         # bbox_logits = model(image, boxes_with_index)\n",
    "#         bbox_logits = model(image, boxes)\n",
    "#         # print(bbox_logits.shape)\n",
    "#         # print(boxes.shape)\n",
    "\n",
    "#         bbox_sizes = (bbox_logits[:, 2] - bbox_logits[:, 0]) * (bbox_logits[:, 3] - bbox_logits[:, 1])\n",
    "\n",
    "#         # print(bbox_logits)\n",
    "#         # print(bbox_sizes)\n",
    "#         # print(bbox_sizes.min())\n",
    "#         # print(bbox_sizes.max())\n",
    "\n",
    "\n",
    "#         scores = 1 - (bbox_sizes - bbox_sizes.min()) / (bbox_sizes.max() - bbox_sizes.min())\n",
    "#         scores_tensor = torch.tensor(scores, dtype=torch.float32, device=device)\n",
    "#         scores_tensor = scores_tensor.squeeze()\n",
    "#         # print(scores_tensor.shape)\n",
    "#         # print(scores_list)\n",
    "\n",
    "#         # print(\"I am score tensor\",scores_list)\n",
    "#         preds_list = []\n",
    "#         target_list = []\n",
    "\n",
    "#         with torch.no_grad():\n",
    "\n",
    "#             for pred_box, target_box, score in zip(bbox_logits, boxes, scores_tensor):\n",
    "#                 # print(pred_box.unsqueeze(0).shape)\n",
    "#                 # print(score.shape)\n",
    "\n",
    "#                 pred_dict = {\n",
    "#                     \"boxes\": pred_box.unsqueeze(0),\n",
    "#                     \"scores\": torch.tensor([score], dtype=torch.float32, device=device),\n",
    "#                     \"labels\": label_tensor.repeat(len(pred_box.unsqueeze(0)))\n",
    "#                 }\n",
    "#                 preds_list.append(pred_dict)\n",
    "#                 # print(label_tensor.shape)\n",
    "\n",
    "#                 # print(target_box.unsqueeze(0).shape)\n",
    "\n",
    "#                 target_dict = {\n",
    "#                     \"boxes\": target_box.unsqueeze(0),\n",
    "#                     \"labels\": label_tensor.repeat(len(target_box.unsqueeze(0)))\n",
    "#                 }\n",
    "#                 target_list.append(target_dict)\n",
    "#                 # print(label_tensor.shape)\n",
    "\n",
    "#         # print(\"I am pred\", preds_list)\n",
    "#         # print(\"I am target\", target_dict)\n",
    "#         map_metric.update(preds=preds_list, target=target_list)\n",
    "\n",
    "#         loss_bbox = smooth_l1_loss(bbox_logits, boxes)\n",
    "#         loss = loss_bbox\n",
    "\n",
    "#         loss.backward()\n",
    "#         running_loss += loss.item()\n",
    "\n",
    "#         optimizer.step()\n",
    "#         scheduler.step()\n",
    "\n",
    "#         loss = loss.detach().cpu().item()\n",
    "#         pbar.set_postfix(loss=loss)\n",
    "\n",
    "#     map_value = map_metric.compute()\n",
    "#     map_metric.reset()\n",
    "\n",
    "#     avg_loss = running_loss / len(train_loader.dataset)\n",
    "\n",
    "#     print(f\"Epoch {epoch+1}, Train Avg. Loss: {avg_loss}, Mean Average Precision: {map_value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Validation\n",
    "# model.eval()  # Set model to evaluation mode\n",
    "# val_loss = 0.0\n",
    "# map_metric.reset()  # Reset Mean Average Precision metric for validation\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for image_dict in val_loader:\n",
    "#         images = []\n",
    "#         boxes = []\n",
    "#         labels = []\n",
    "\n",
    "#         for label_id, data in image_dict.items():\n",
    "#             for image, box in zip(data['images'], data['bboxes']):\n",
    "#                 images.append(image)\n",
    "#                 boxes.append(box)\n",
    "#                 labels.append(label_id)\n",
    "\n",
    "#         images_batch = torch.stack(images).to(device)  # Stack images into a batch\n",
    "#         boxes_batch = torch.cat(boxes).to(device)     # Concatenate bounding boxes into a batch\n",
    "#         labels_batch = torch.tensor(labels).to(device)  # Convert labels to tensor\n",
    "\n",
    "#         # Forward pass\n",
    "#         cls_logits, bbox_logits = model(images_batch, boxes_batch)\n",
    "\n",
    "#         # Calculate classification and bounding box regression losses\n",
    "#         loss_cls = F.cross_entropy(cls_logits, labels_batch)\n",
    "#         loss_bbox = smooth_l1_loss(bbox_logits, boxes_batch)\n",
    "\n",
    "#         # Update validation loss\n",
    "#         val_loss += (loss_cls + loss_bbox).item()\n",
    "\n",
    "#         # Compute accuracy\n",
    "#         _, predicted = torch.max(cls_logits, 1)\n",
    "#         correct = (predicted == labels_batch).sum().item()\n",
    "#         accuracy = correct / labels_batch.size(0)\n",
    "#         val_accuracy += accuracy\n",
    "\n",
    "#         # Update Mean Average Precision metric\n",
    "#         preds_list = []\n",
    "#         target_list = []\n",
    "\n",
    "#         # Assume single class for simplicity\n",
    "#         class_label = 0\n",
    "\n",
    "#         for pred_box, label in zip(bbox_logits, labels):\n",
    "#             pred_dict = {\n",
    "#                 \"boxes\": pred_box.unsqueeze(0),  # unsqueeze to add batch dimension\n",
    "#                 \"scores\": torch.ones(1).to(device),  # placeholder scores\n",
    "#                 \"labels\": torch.tensor([class_label], dtype=torch.int64).to(device)\n",
    "#             }\n",
    "#             preds_list.append(pred_dict)\n",
    "\n",
    "#             target_dict = {\n",
    "#                 \"boxes\": label.unsqueeze(0),  # unsqueeze to add batch dimension\n",
    "#                 \"labels\": torch.tensor([class_label], dtype=torch.int64).to(device)\n",
    "#             }\n",
    "#             target_list.append(target_dict)\n",
    "\n",
    "#         # Update the Mean Average Precision metric\n",
    "#         map_metric.update(preds=preds_list, target=target_list)\n",
    "\n",
    "# # Calculate Mean Average Precision\n",
    "# map_value = map_metric.compute()\n",
    "\n",
    "# # Calculate average validation loss and accuracy per epoch\n",
    "# avg_val_loss = val_loss / len(val_loader.dataset)\n",
    "\n",
    "# print(f\"Epoch {epoch+1}, Validation Avg. Loss: {avg_val_loss}, Mean Average Precision: {map_value}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ADL_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
